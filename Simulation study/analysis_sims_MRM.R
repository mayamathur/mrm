


################################## PRELIMINARIES ##################################

rm(list=ls())

library(renv)
# renv::snapshot()
# renv::restore()

library(dplyr)
library(xtable)
library(data.table)
library(tibble)
library(testthat)
library(ggplot2)
library(sandwich)

options(scipen=999)

prepped.data.dir = "~/Dropbox/Personal computer/Independent studies/2020/Meta-regression metrics (MRM)/Simulation study results/*Main-text sims/Prepped data"
results.dir = "~/Dropbox/Personal computer/Independent studies/2020/Meta-regression metrics (MRM)/Simulation study results/*Main-text sims/Results from R"
figures.results.dir = "~/Dropbox/Personal computer/Independent studies/2020/Meta-regression metrics (MRM)/Simulation study results/*Main-text sims/Results from R/Figures"
code.dir = "~/Dropbox/Personal computer/Independent studies/2020/Meta-regression metrics (MRM)/Code (git)/Simulation study"
overleaf.dir = "~/Dropbox/Apps/Overleaf/Moderators in meta-regression (MRM)/supp_figures_autogenerated"


setwd(prepped.data.dir)
# data aggregated by scenario
agg = fread("*agg_dataset.csv")
expect_equal( 4679, nrow(agg) ) # compare to data prep script
# data for each simulation iterate
s = fread("s3_dataset.csv")

setwd(code.dir)
source("helper_MRM.R")

# takes about 5 min
regressions.from.scratch = TRUE

# should we wipe all existing violin plots?
redo.violins = TRUE

# make datasets corresponding to recommended scenarios in Discussion
# recommended scenarios for Phat
aggPhat = make_agg_data( s %>% filter( !(clustered == TRUE & true.effect.dist == "expo") ) )
expected_equal( nrow(aggPhat), 3522 )
# save it
setwd(prepped.data.dir)
fwrite(aggPhat, "*agg_dataset_recommended_scens_phat.csv")

# recommended scenarios for Diff
aggDiff = make_agg_data( s %>% filter(contrast != "BC-rare" &
                                        !(clustered == TRUE & true.effect.dist == "expo") &
                                        k >= 20 ) )
expected_equal( nrow(aggDiff), 1908 )
# save it
setwd(prepped.data.dir)
fwrite(aggDiff, "*agg_dataset_recommended_scens_diff.csv")

################################## GENERAL STATS ON SIMULATIONS ##################################

##### I^2 Parameterization of Heterogeneity #####

# for each possible sample size in our sims, convert all the heterogeneity 
#  parameters to I^2
table(s$muN)
table(s$V)

round( I2( t2 = c(0.0025, .01, .04, .25, .64),
           N = 100), 2 )

round( I2( t2 = c(0.0025, .01, .04, .25, .64),
           N = 850), 2 )
# so the tau = 0.5 (t2 = 0.25) scenarios are approximately I^2 = 50%
# which is exactly Higgins' (2003) benchmark for "moderate" heterogeneity :)

##### ICC in Clustered Scenarios #####
agg %>% filter(clustered == TRUE) %>%
  summarise(min(ICCpop),
            mean(ICCpop),
            median(ICCpop),
            max(ICCpop))

##### Number and Percentage of Successful Reps #####

# total scenarios
# this is 2X the number of data-generation parameter combinations because 
#  each has both MR and DL rows
nrow(agg)
# 2400: number of unique data-generation parameters
nrow(agg) / (2400 * 2)
# proportion of scenarios that didn't run
1 - (nrow(agg) / (2400 * 2))

table(agg$calib.method)

data.gen.params = c( "k",
                     "V",
                     "Vzeta",
                     "minN",
                     "true.effect.dist",
                     "TheoryP",
                     "contrast")

# number of unique data generation parameters actually represented in
#  the results
x = s %>% group_by_at(data.gen.params) %>%
  summarise(meanNA(Phat))
nrow(x)  # 2388 of 2400 possible

##### Rep Time #####
# this variable is the number of seconds for the ENTIRE loop in each doParallel file, so represents the time to complete sim.reps=500 parallelized iterates 
# so this is basically the time to run each scenario

# express in total days of simulation time across all nodes
totalSimTime = sum(agg$repTime)
secondsPerDay = 86400
totalSimTime / secondsPerDay
# 79 days of computational time


################################## ONE-STAGE VS. TWO-STAGE ##################################

param.vars = c("calib.method.pretty",
               "k",
               "V",
               "Vzeta",
               "minN",
               "true.effect.dist",
               "TheoryP",
               "contrast")

outcomes = c("PhatRelBias", "CoverPhat", "DiffRelBias",  "CoverDiff", "PhatCIWidth", "DiffCIWidth")

# sanity check:
# make sure we listed all the param vars
t = s %>% group_by_at(param.vars) %>%
  summarise( n = n(),
             .groups = "keep" )
# 500 such that each scenario is uniquely defined by the param vars
expect_equal( unique(t$n), 500 )


# compare one-stage to two-stage method
t = s %>% group_by_at(param.vars) %>%
  group_by(calib.method.pretty) %>%
  summarise_at( .vars = outcomes,
                .funs = meanNA )
View(t)  



################################## SUPPLEMENT TABLE: REGRESS PERFORMANCE METRICS ON OBSERVED STATS ##################################

# regressions are fit at iterate level, so can't use abs bias here
#  since it's only defined at scenario level
outcomes = c("PhatAbsErr",
             "CoverPhat",
             "PhatCIWidth",
             
             "DiffAbsErr",
             "CoverDiff",
             "DiffCIWidth")

s$contrast.extreme = s$contrast == "BC-rare"

# takes about 2-3 min
if ( regressions.from.scratch == TRUE ) {
  
  # at scenario level rather than individual iterate level
  for (i in outcomes){
    
    # same predictor variables as in violin plots
    # but don't use the contrast for Phat because it's not relevant
    if ( grepl( x=i, pattern="Diff" ) ) obsVars = c("k", "muN", "clustered", "true.effect.dist", "contrast.extreme")
    if ( grepl( x=i, pattern="Phat" ) ) obsVars = c("k", "muN", "clustered", "true.effect.dist")
    
    string = paste( i, "~", paste(obsVars, collapse = "+"), sep="" )
    mod = lm( eval(parse(text=string)),
              data = s )
    
    SEs = diag( vcovCL(mod, type="HC0", cluster = ~ scen.name) )
    SEs = SEs[ !names(SEs) == "(Intercept)" ]
    
    coefs = mod$coefficients[ !names(mod$coefficients) == "(Intercept)" ]
    z = coefs/SEs
    pvals = 2 * ( 1 - pnorm( abs(z) ) )
    
    # which vars are good vs. bad for the outcome?
    # flip coeff signs so that positive means it improves the outcome
    if ( grepl(pattern = "AbsErr", x = i) | grepl(pattern = "Width", x = i) ) coefs = -coefs
    good = names( coefs[ coefs > 0 & pvals < 0.001 ] )
    bad = names( coefs[ coefs < 0 & pvals < 0.001 ] )
    
    good = paste(good, collapse = ", ")
    bad = paste(bad, collapse = ", ")
    
    newRow = data.frame( outcome = i,
                         good = good, 
                         bad = bad )
    
    if (i==outcomes[1]) res = newRow else res = rbind(res, newRow)
    
    cat( paste("\n\n*******************", toupper(i), " PREDICTORS*******************\n" ) )
    print(summary(mod))
    
    
    #   # best subsets
    #   # ex on page 298 here is good: https://journal.r-project.org/archive/2018/RJ-2018-059/RJ-2018-059.pdf
    #   library(rFSA)
    #   string = paste( i, " ~ 1", sep="" )
    #   keepers = c(obsVars, i)
    #   mod2 = FSA( formula = eval( parse(text = string) ),
    #               #data = s[1:1000,] %>% select(keepers),  # for testing
    #               data = s %>% select(keepers),
    #               cores = 8,
    #               m = 2,  # order of interactions to try
    #               interactions = FALSE,
    #               criterion = AIC)
    #   mod2
    #   
    #   if ( i == outcomes[1] ) bestMod = list( summary(mod2)[[2]] ) else bestMod[[ length(bestMod) + 1 ]] = summary(mod2)[[2]]
    
    
  }  # end loop over outcomes
  
  
  # look at results
  res
  
  # clean up string formatting
  res2 = res %>% mutate_at( vars(good, bad), my_recode )
  
  # write as csv
  setwd(results.dir)
  setwd("Tables to prettify")
  write.csv(res2, "performance_predictors.csv")
  
  # also print as xtable for Overleaf
  
  print( xtable(res2), include.rownames = FALSE )
  
}


################################## FIGURES ##################################

# how this section works:
# this goes through all the outcomes and observable characteristics, 
#  creates a plot and saves it to both Overleaf and the results.dir,
#  and then auto-generates \includegraphics strings for Overleaf for 
#  each figure (which you can then copy-paste straight into Overleaf)
#  if you don't want to change the figure sizes or names, no need to copy-paste again

# wipe existing files
if ( redo.violins == TRUE ) {
  setwd(figures.results.dir)
  setwd("1. Good scenarios (marginal)")
  system("rm *")
  
  setwd(figures.results.dir)
  setwd("2. All scenarios (marginal)")
  system("rm *")
  
  setwd(figures.results.dir)
  setwd("3. All scenarios (stratified)")
  system("rm *")
  
  setwd(figures.results.dir)
  setwd("4. Good scenarios (stratified by k)")
  system("rm *")
  
  
  setwd(overleaf.dir)
  system("rm *")
}



##### 1. Marginal Violin Plots - Scenarios Fulfilling Guidelines in Discussion #####


# put these in the order the plots should appear in manuscript
outcomes = c("PhatBias",
             "PhatAbsErr",
             "CoverPhat",
             "PhatCIWidth",
             
             "DiffBias",
             "DiffAbsErr",
             "CoverDiff",
             "DiffCIWidth")

# for ordering files
myLetters  = 1:200

# this will be incremented so that each plot's title is prefaced
#  by a letter to force the correct ordering
alphaIndex = 1

# to avoid subsequent error about "cannot change value of locked binding for 'ylab'"
ylab = NULL


for ( y in outcomes ) {
  
  yTicks = NA
  
  # for violin plots, set global parameters that my_violins() will use as arguments
  set_violin_params()
  
  # sanity check
  # aggData is set by set_violin_params
  if( grepl(x = y, pattern = "Diff")) expect_equal( nrow(aggData), 1908 )
  if( grepl(x = y, pattern = "Phat")) expect_equal( nrow(aggData), 3522 )
  
  my_violins( 
    yName = y,
    hline = hline,
    ylab = ylab,
    yTicks = yTicks,
    prefix = paste( myLetters[alphaIndex], "goodScens", sep = "_" ),
    .results.dir = paste( figures.results.dir, "/1. Good scenarios (marginal)", sep = "" ),
    .agg = aggData )
  
  cat( "\n Just finished marginal", y, ", oh yeah" )
  
  alphaIndex = alphaIndex + 1
  
}  # end loop over Y


# auto-generate figure strings for Overleaf
setwd( paste( figures.results.dir, "/1. Good scenarios (marginal)", sep = "" ) )
overleaf_figure_strings()


##### 2. Marginal Violin Plots - All Scenarios #####

# this will be incremented so that each plot's title is prefaced
#  by a letter to force the correct ordering

for ( y in outcomes ) {
  
  yTicks = NA
  
  set_violin_params()
  
  my_violins( 
    yName = y,
    hline = hline,
    ylab = ylab,
    yTicks = yTicks,
    prefix = myLetters[alphaIndex],
    .agg = agg,
    .results.dir = paste( figures.results.dir, "/2. All scenarios (marginal)", sep = "" ) )
  
  cat( "\n Just finished marginal", y, ", oh yeah" )
  
  alphaIndex = alphaIndex + 1
  
}  # end loop over Y



# auto-generate figure strings for Overleaf
setwd( paste( figures.results.dir, "/2. All scenarios (marginal)", sep = "" ) )
overleaf_figure_strings()



##### 3. Violin Plots by Meta-Analysis Characteristics - All Scenarios #####

# pretty variable names for X-axis
agg$true.effect.dist.pretty = dplyr::recode( agg$true.effect.dist,
                                             expo = "Exponential",
                                             normal = "Normal")

agg$clustered.pretty = dplyr::recode( agg$clustered,
                                      `0` = "Not clustered",
                                      `1` = "Clustered")


for ( y in outcomes ) {
  
  yTicks = NA
  
  # simplified list of categorical observed variables
  # include contrast only if outcome is difference
  if( grepl(x = y, pattern = "Diff")) categX = c("k", "muN", "clustered.pretty", "true.effect.dist.pretty", "contrast")
  
  if( grepl(x = y, pattern = "Phat")) categX = c("k", "muN", "clustered.pretty", "true.effect.dist.pretty")
  
  
  set_violin_params()
  
  
  for ( x in categX ) {
    
    # set up x-labels
    if ( x == "k" ) xlab = "Number of studies (k)"
    if ( x == "muN" ) xlab = "Average sample size (E[N])"
    if ( x == "EstVar" ) xlab = "Estimated residual heterogeneity"
    if ( x == "clustered.pretty" ) xlab = "Clustering of population effects"
    if ( x == "true.effect.dist.pretty" ) xlab = "Distribution of population effects"
    if ( x == "TheoryP" ) xlab = "True proportion"
    if ( x == "TheoryDiff" ) xlab = "True difference"
    if ( x == "contrast" ) xlab = "Covariate contrast"
    
    my_violins( xName = x,
                yName = y,
                hline = hline,
                xlab = xlab,
                ylab = ylab,
                yTicks = yTicks,
                prefix = myLetters[alphaIndex],
                .agg = agg,
                .results.dir = paste( figures.results.dir, "/3. All scenarios (stratified)", sep = "" ) )
    
    cat( "\n Just finished", x, "vs", y, ", oh yeah" )
    
    alphaIndex = alphaIndex + 1
    
  }  # end loop over X
  
  
}  # end loop over Y


# auto-generate figure strings for Overleaf
setwd( paste( figures.results.dir, "/3. All scenarios (stratified)", sep = "" ) )
overleaf_figure_strings()


##### 4. (Main Text) Violin Plots by k Only - Good Scenarios #####

# start index over because these are going in main text
# start at 4 because we already have figures 1-3
index = 4



for ( y in outcomes ) {
  
  yTicks = NA
  
  # simplified list of categorical observed variables
  # include contrast only if outcome is difference
  if( grepl(x = y, pattern = "Diff")) categX = c("k", "muN", "clustered.pretty", "true.effect.dist.pretty", "contrast")
  
  if( grepl(x = y, pattern = "Phat")) categX = c("k", "muN", "clustered.pretty", "true.effect.dist.pretty")
  
  
  set_violin_params()
  
  # sanity check
  # aggData is set by set_violin_params
  if( grepl(x = y, pattern = "Diff")) expect_equal( nrow(aggData), 1908 )
  if( grepl(x = y, pattern = "Phat")) expect_equal( nrow(aggData), 3522 )
  
  xlab = "Number of studies (k)"
  
  my_violins( xName = "k",
              yName = y,
              hline = hline,
              xlab = xlab,
              ylab = ylab,
              yTicks = yTicks,
              prefix = paste("Fig", index),
              .agg = aggData,
              .results.dir = paste( figures.results.dir, "/4. Good scenarios (stratified by k)", sep = "" ) )
  
  cat( "\n Just finished k vs", y, ", oh yeah" )
  
  index = index + 1
  
  
}  # end loop over Y


# # auto-generate figure strings for Overleaf
# setwd( paste( figures.results.dir, "/4. Good scenarios (stratified by k)", sep = "" ) )
# overleaf_figure_strings()





################################## TABLES 4-5 FOR PAPER, WITH RULES OF THUMB ##################################

# choose which average to take across scenarios ("median", "mean", "median.pctiles")
# with 10th and 90th percentiles
averagefn = "median.pctiles"


##### Table 4: Phat #####

# make filtered dfs (will form rows of table)
agg2 = make_agg_data( s %>% filter(contrast != "BC-rare") )
agg3 = make_agg_data( s %>% filter(true.effect.dist == "normal") )

selectVars = "Phat"

t1 = rbind( my_summarise(dat = agg,
                         description = "All reps",
                         averagefn = averagefn),
            
            # **this one gives 0% chance of coverage<85%
            my_summarise(dat = agg3,
                         description = "Normal",
                         averagefn = averagefn),
            
            # **this one gives 1% chance of coverage<85%
            my_summarise(dat = aggPhat,
                         description = "Not clustered expo",
                         averagefn = averagefn) )  


# save pretty table for paper
keepers = c("Scenarios",
            "n.scens", 
            "PhatBias",
            "PhatAbsErr",
            "PhatRelBias",
            
            "EstMeanRelBias",
            "EstVarRelBias",
            
            "CoverPhat",
            "BadPhatCover",
            "PhatCIWidth",
            "BadPhatWidth")
setwd(results.dir)
setwd("Tables to prettify")
write.csv(t1 %>% select(keepers), "Phat_results_table.csv")

View( t1 %>% select(keepers) )


##### Table 5: Diff #####

# recommended scenarios for diff
aggDiff = make_agg_data( s %>% filter(contrast != "BC-rare" &
                                        !(clustered == TRUE & true.effect.dist == "expo") &
                                        k >= 20) )

# tried but not useful:
#aggDiff = make_agg_data( s %>% filter(contrast != "BC-rare" & k>=100 ) )

# agg6 = make_agg_data(s %>% filter(k >= 100) )
# 
# agg7 = make_agg_data(s %>% filter( k >= 100 &
#                                      !(clustered == TRUE & true.effect.dist == "expo") ) ) 
# 
# agg9 = make_agg_data(s %>% filter( k >= 100 &
#                                      true.effect.dist == "normal" ) ) 
# 
# agg10 = make_agg_data(s %>% filter( k >= 100 &
#                                       muN==850 &
#                                       true.effect.dist == "normal" &
#                                       clustered == FALSE) ) 


#bm
selectVars = "Diff"

t2 = rbind( my_summarise(dat = agg,
                         description = "All reps",
                         averagefn = averagefn),
            
            my_summarise(dat = agg3,
                         description = "Normal",
                         averagefn = averagefn),
            
            
            my_summarise(dat = agg2,
                         description = "Not BC-rare",
                         averagefn = averagefn),
            
            
            my_summarise(dat = aggDiff,
                         description = "Not BC-rare/not clustered expo/k >= 20",
                         averagefn = averagefn)
) 


# save pretty table for paper
keepers = c("Scenarios",
            "n.scens", 
            "DiffBias",
            "DiffAbsErr",
            "DiffRelBias",
            "EstMeanRelBias",
            "EstVarRelBias",
            
            "CoverDiff",
            "BadDiffCover",
            "DiffCIWidth",
            "BadDiffWidth")
setwd(results.dir)
setwd("Tables to prettify")
write.csv(t2 %>% select(keepers), "diff_results_table.csv")

View( t2 %>% select(keepers) )


###### All Metrics for All of the Filtered Datasets (Online Dataset) #####

# diff, phat, logitphat, etc.

selectVars = "all"
t3 = rbind( my_summarise(dat = agg,
                         description = "All reps",
                         averagefn = averagefn),
            
            my_summarise(dat = agg2,
                         description = "No BC-rare",
                         averagefn = averagefn),
            
            my_summarise(dat = agg3,
                         description = "Normal",
                         averagefn = averagefn),
            
            my_summarise(dat = aggPhat,
                         description = "Not clustered expo",
                         averagefn = averagefn),
            
            my_summarise(dat = aggDiff,
                         description = "No BC-rare/not clustered expo/k>=50",
                         averagefn = averagefn) ) 


setwd(results.dir)
write.csv(t3, "extended_performance_table.csv")



# ################################## ONLY FOR RSM_3 RESPONSE LETTER: COVERAGE WITH K<150 ##################################
# 
# 
# # not in text
# 
# ##### Phat #####
# # like aggPhat, but with k<150: still 1% with bad coverage
# temp = make_agg_data( s %>% filter( !(clustered == TRUE & true.effect.dist == "expo") & k < 150 ) )
# res = my_summarise(dat = temp, averagefn = averagefn, .selectVars = "Phat")
# res$BadPhatCover
# res$CoverPhat
# 
# # # compare to k=150 only: 3% bad coverage and "0.93 (0.88, 0.97)"
# # temp = make_agg_data( s %>% filter( !(clustered == TRUE & true.effect.dist == "expo") & k == 150 ) )
# # res = my_summarise(dat = temp, averagefn = averagefn)
# # res$BadPhatCover
# # res$CoverPhat
# 
# 
# ##### Diff #####
# # like aggDiff, but with k<150
# temp = make_agg_data( s %>% filter( contrast != "BC-rare" &
#                                      !(clustered == TRUE & true.effect.dist == "expo") & k < 150 ) )
# res = my_summarise(dat = temp, averagefn = averagefn, .selectVars = "Diff")
# res$BadDiffCover
# res$CoverDiff
# 
# 
# ################################## ONLY FOR RSM_4 RESPONSE LETTER: CI WIDTH WHEN K<50 ##################################
# 
# # these are in text (in-line)
# 
# ##### Phat #####
# # like aggPhat, but with k<150: still 1% with bad coverage
# temp = make_agg_data( s %>% filter( !(clustered == TRUE & true.effect.dist == "expo") & k <= 20 ) )
# res = my_summarise(dat = temp, averagefn = averagefn, .selectVars = "Phat")
# res$BadPhatCover
# res$CoverPhat
# res$PhatCIWidth
# res$BadPhatWidth
# 
# # # compare to k=150 only: 3% bad coverage and "0.93 (0.88, 0.97)"
# # temp = make_agg_data( s %>% filter( !(clustered == TRUE & true.effect.dist == "expo") & k == 150 ) )
# # res = my_summarise(dat = temp, averagefn = averagefn)
# # res$BadPhatCover
# # res$CoverPhat
# 
# 
# ##### Diff #####
# # like aggDiff, but with k<150
# temp = make_agg_data( s %>% filter( contrast != "BC-rare" &
#                                       !(clustered == TRUE & true.effect.dist == "expo") & k <= 20 ) )
# res = my_summarise(dat = temp, averagefn = averagefn, .selectVars = "Diff")
# res$BadDiffCover
# res$CoverDiff
# res$DiffCIWidth
# res$BadDiffWidth
# 
# 
# ################################## ONLY FOR RSM_5 RESPONSE LETTER: COVERAGE WHEN K=150 ##################################
# 
# ##### Phat #####
# # like aggPhat, but with k=150: still 1% with bad coverage
# temp = make_agg_data( s %>% filter( !(clustered == TRUE & true.effect.dist == "expo") & k == 150 ) )
# res = my_summarise(dat = temp, averagefn = averagefn, .selectVars = "Phat")
# res$BadPhatCover
# res$CoverPhat
# 
# # # compare to k=150 only: 3% bad coverage and "0.93 (0.88, 0.97)"
# # temp = make_agg_data( s %>% filter( !(clustered == TRUE & true.effect.dist == "expo") & k == 150 ) )
# # res = my_summarise(dat = temp, averagefn = averagefn)
# # res$BadPhatCover
# # res$CoverPhat
# 
# 
# ##### Diff #####
# # like aggDiff, but with k<150
# temp = make_agg_data( s %>% filter( contrast != "BC-rare" &
#                                       !(clustered == TRUE & true.effect.dist == "expo") & k == 150 ) )
# res = my_summarise(dat = temp, averagefn = averagefn, .selectVars = "Diff")
# res$BadDiffCover
# res$CoverDiff


################################## ONLY FOR RSM_5 RESPONSE LETTER: MIN/MAX ISSUE ##################################

# compare max bias to standard estimators

min(aggPhat$PhatBias)
max(aggPhat$PhatBias)

max(aggPhat$PhatRelBias)
max(aggPhat$EstMeanRelBias)
max(aggPhat$EstVarRelBias)

temp = aggPhat[ which.max(aggPhat$PhatRelBias), ]
temp$Phat
temp$TheoryP
temp$EstVar
temp$TrueVar

min(aggDiff$DiffBias)
max(aggDiff$DiffBias)

max(aggDiff$DiffRelBias)
max(aggDiff$EstVarRelBias)

# worst bias in estimated mean
max( abs(agg$EstMean - agg$TrueMean ) )


# ################################## RELATIVE BIAS WHEN THEORYP <= 0.20 ##################################
# 
# ### Phat
# # all scenarios
# temp = my_summarise(dat = agg %>% filter( TheoryP >= 0.2 ),
#                     averagefn = averagefn,
#                     .selectVars = "Phat")
# 
# temp %>% select( "PhatRelBias", "PhatAbsErr" )
# 
# 
# ### Diff
# # all scenarios
# temp = my_summarise(dat = agg %>% filter( TheoryDiff >= 0.2 ),
#                     .selectVars = "Diff",
#                     averagefn = averagefn)
# 
# temp %>% select( "DiffRelBias", "DiffAbsErr" )
# 
# 
# #### Look at the Worst 10% of Recommended Scenarios WRT Abs and Rel Bias #####
# 
# ### Phat
# # use only recommended scenarios that had absolute bias in worst 10%
# ( q90 = quantile( aggPhat$PhatAbsErr, 0.90 ) )
# 
# tempAgg = aggPhat %>% filter( PhatAbsErr > q90 )
# 
# tempRes = my_summarise(dat = tempAgg,
#                     averagefn = averagefn,
#                     .selectVars = "Phat")
# 
# tempRes %>% select( "PhatAbsErr", "PhatCIWidth", "CoverPhat" )
# 
# # what proportion of these scenarios had highly uninformative CIs?
# mean( tempAgg$PhatCIWidth > 0.9 )
# 
# ### Diff
# # use only recommended scenarios
# ( q90 = quantile( aggDiff$DiffAbsErr, 0.90 ) )
# 
# tempAgg = aggDiff %>% filter( DiffAbsErr > q90 )
# 
# tempRes = my_summarise(dat = tempAgg,
#                        averagefn = averagefn,
#                        .selectVars = "Diff")
# 
# tempRes %>% select( "DiffAbsErr", "DiffCIWidth", "CoverDiff" )
# 
# # what proportion of these scenarios had highly uninformative CIs?
# mean( tempAgg$DiffCIWidth > 0.9 )
# 



################################## BIAS-CORRECT META-REGRESSION ESTIMATES IN WORST SCENARIOS ##################################

setwd("~/Dropbox/Personal computer/Independent studies/2020/Meta-regression metrics (MRM)/Simulation study results/Supplement - boot-correct meta-regression")

# "b" for "bias correction"
s2b = fread("stitched.csv")
s2b$contrast = "BC-rare"

s3b = make_s3_data(s2b)


# has one row per scenario, so 3 rows per scen.name.in.main
aggb = make_agg_data(s3b)

selectVars = "all"

t = data.frame( rbind( my_summarise(dat = aggb %>% filter(calib.method == "MR"),
                                    description = "MR"),
                       
                       my_summarise(dat = aggb %>% filter(calib.method == "MR bt both correct"),
                                    description = "Bt correct"),
                       
                       my_summarise(dat = aggb %>% filter(calib.method == "params"),
                                    description = "params") ) )

t = t %>% select("Scenarios", "PhatBias", "PhatAbsErr",  "PhatRelBias", "DiffBias", "DiffAbsErr", "DiffRelBias")
t

# yes, the bias corrections make the bias worse...

setwd(results.dir)
setwd("Tables to prettify")
fwrite(t, "supplement_meta_regression_corrections.csv")






