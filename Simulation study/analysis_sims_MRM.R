
################################## PRELIMINARIES ##################################

rm(list=ls())

library(dplyr)
library(xtable)
library(data.table)
library(tibble)
library(testthat)

options(scipen=999)

prepped.data.dir = "~/Dropbox/Personal computer/Independent studies/2020/Meta-regression metrics (MRM)/Simulation study results/*Main-text sims/Prepped data"
results.dir = "~/Dropbox/Personal computer/Independent studies/2020/Meta-regression metrics (MRM)/Simulation study results/*Main-text sims/Results from R"
figures.results.dir = "~/Dropbox/Personal computer/Independent studies/2020/Meta-regression metrics (MRM)/Simulation study results/*Main-text sims/Results from R/Figures"
code.dir = "~/Dropbox/Personal computer/Independent studies/2020/Meta-regression metrics (MRM)/Code (git)/Simulation study"
overleaf.dir = "~/Dropbox/Apps/Overleaf/Moderators in meta-regression (MRM)/supp_figures_autogenerated"


setwd(prepped.data.dir)
# data aggregated by scenario
agg = fread("*agg_dataset.csv")
expect_equal( 4679, nrow(agg) ) # compare to data prep script
# data for each simulation iterate
s = fread("s3_dataset.csv")

setwd(code.dir)
source("helper_MRM.R")
source("analysis_sims_helper_MRM.R")

# takes about 5 min
regressions.from.scratch = FALSE



################################## GENERAL STATS ON SIMULATIONS ##################################

##### I^2 Parameterization of Heterogeneity #####

# for each possible sample size in our sims, convert all the heterogeneity 
#  parameters to I^2
table(s$muN)
table(s$V)

round( I2( t2 = c(0.0025, .01, .04, .25, .64),
           N = 100), 2 )

round( I2( t2 = c(0.0025, .01, .04, .25, .64),
           N = 850), 2 )
# so the tau = 0.5 (t2 = 0.25) scenarios are approximately I^2 = 50%
# which is exactly Higgins' (2003) benchmark for "moderate" heterogeneity :)

##### ICC in Clustered Scenarios #####
agg %>% filter(clustered == TRUE) %>%
  summarise(min(ICCpop),
            mean(ICCpop),
            median(ICCpop),
            max(ICCpop))

##### Number and Percentage of Successful Reps #####

# total scenarios
# this is 2X the number of data-generation parameter combinations because 
#  each has both MR and DL rows
nrow(agg)
# 2400: number of unique data-generation parameters
nrow(agg) / (2400 * 2)
# proportion of scenarios that didn't run
1 - (nrow(agg) / (2400 * 2))

table(agg$calib.method)

data.gen.params = c( "k",
                     "V",
                     "Vzeta",
                     "minN",
                     "true.effect.dist",
                     "TheoryP",
                     "contrast")

# number of unique data generation parameters actually represented in
#  the results
x = s %>% group_by_at(data.gen.params) %>%
  summarise(meanNA(Phat))
nrow(x)  # 2388 of 2400 possible

##### Rep Time #####
# this variable is the number of seconds for the ENTIRE loop in each doParallel file, so represents the time to complete sim.reps=500 parallelized iterates 
# so this is basically the time to run each scenario

# express in total days of simulation time across all nodes
totalSimTime = sum(agg$repTime)
secondsPerDay = 86400
totalSimTime / secondsPerDay
# 79 days of computational time


################################## ONE-STAGE VS. TWO-STAGE ##################################

param.vars = c("calib.method.pretty",
               "k",
               "V",
               "Vzeta",
               "minN",
               "true.effect.dist",
               "TheoryP",
               "contrast")

outcomes = c("PhatRelBias", "CoverPhat", "DiffRelBias",  "CoverDiff", "PhatCIWidth", "DiffCIWidth")

# sanity check:
# make sure we listed all the param vars
t = s %>% group_by_at(param.vars) %>%
  summarise( n = n(),
             .groups = "keep" )
# 500 such that each scenario is uniquely defined by the param vars
expect_equal( unique(t$n), 500 )


# compare one-stage to two-stage method
t = s %>% group_by_at(param.vars) %>%
  group_by(calib.method.pretty) %>%
  summarise_at( .vars = outcomes,
                .funs = meanNA )
View(t)  



################################## REGRESS PERFORMANCE METRICS ON OBSERVED STATS ##################################

# focus on observable variables within scenarios
# i.e., estimates rather than parameters
obsVars = c("k", "muN", "Phat", "PhatRef", "EstMean", "EstVar", "PhatBtFail", "calib.method.pretty",
            # last two are only somewhat observed:
            "true.effect.dist", "clustered")

outcomes = c("PhatRelBias", "CoverPhat", "DiffRelBias",  "CoverDiff")

# takes about 5 min
if ( regressions.from.scratch == TRUE ) {
  # at scenario level rather than individual iterate level
  for (i in outcomes){
    string = paste( i, "~", paste(obsVars, collapse = "+"), sep="" )
    mod = lm( eval(parse(text=string)),
              data = s )
    
    library(sandwich)
    SEs = diag( vcovCL(mod, type="HC0", cluster = ~ scen.name) )
    SEs = SEs[ !names(SEs) == "(Intercept)" ]
    
    coefs = mod$coefficients[ !names(mod$coefficients) == "(Intercept)" ]
    z = coefs/SEs
    pvals = 2 * ( 1 - pnorm( abs(z) ) )
    
    # which vars are good vs. bad for the outcome?
    # flip coeff signs so that positive means it improves the outcome
    if ( grepl(pattern = "Bias", x = i) ) coefs = -coefs
    good = names( coefs[ coefs > 0 & pvals < 0.001 ] )
    bad = names( coefs[ coefs < 0 & pvals < 0.001 ] )
    
    good = paste(good, collapse = ",")
    bad = paste(bad, collapse = ",")
    
    newRow = data.frame( outcome = i,
                         good = good, 
                         bad = bad )
    
    if (i==outcomes[1]) res = newRow else res = rbind(res, newRow)
    
    cat( paste("\n\n*******************", toupper(i), " PREDICTORS*******************\n" ) )
    print(summary(mod))
    
    
    # best subsets
    # ex on page 298 here is good: https://journal.r-project.org/archive/2018/RJ-2018-059/RJ-2018-059.pdf
    library(rFSA)
    string = paste( i, " ~ 1", sep="" )
    keepers = c(obsVars, i)
    mod2 = FSA( formula = eval( parse(text = string) ),
                #data = s[1:1000,] %>% select(keepers),  # for testing
                data = s %>% select(keepers),
                cores = 8,
                m = 2,  # order of interactions to try
                interactions = FALSE,
                criterion = AIC)
    mod2
    
    if ( i == outcomes[1] ) bestMod = list( summary(mod2)[[2]] ) else bestMod[[ length(bestMod) + 1 ]] = summary(mod2)[[2]]
  }
  
  
  # look at results
  res
  
  bestMod
  
  setwd(results.dir)
  write.csv(res, "performance_predictors.csv")
}


################################## FIGURES ##################################

# how this section works:
# this goes through all the outcomes and observable characteristics, 
#  creates a plot and saves it to both Overleaf and the results.dir,
#  and then auto-generates \includegraphics strings for Overleaf for 
#  each figure (which you can then copy-paste straight into Overleaf)
#  if you don't want to change the figure sizes or names, no need to copy-paste again

# wipe existing files
setwd(figures.results.dir)
system("rm *")
setwd(overleaf.dir)
system("rm *")


##### Marginal Violin Plots - Scenarios Fulfilling Guidelines in Discussion #####

agg5 = make_agg_data( s %>% filter(contrast != "BC-rare" &
                                     !(clustered == TRUE & true.effect.dist == "expo") ) )

# put these in the order they should appear in manuscript
outcomes = c("PhatRelBias", "CoverPhat", "DiffRelBias",  "CoverDiff")

# this will be incremented so that each plot's title is prefaced
#  by a letter to force the correct ordering
alphaIndex = 1

for ( y in outcomes ) {
  
  yTicks = NA
  
  # set up y-labels and hline
  if ( y == "PhatRelBias" ) {
    ylab = "Relative bias in estimated proportion above q"
    hline = 0
    yTicks = seq(0, 5, .5)
    
    #cat("\\subsubsection{Relative bias in est prop}")
  }
  
  if ( y == "DiffRelBias" ) {
    ylab = "Relative bias in estimated difference in proportions"
    hline = 0
    yTicks = seq(0, 5, .5)
  }
  
  if ( y == "CoverPhat" ) {
    ylab = "95% CI coverage for estimated proportion above q"
    hline = 0.95
    yTicks = seq(0, 1, 0.1)
  }
  
  if ( y == "CoverDiff" ) {
    ylab = "95% CI coverage of estimated difference in proportions"
    hline = 0.95
    yTicks = seq(0, 1, 0.1)
  }
  
  my_violins( 
    yName = y,
    hline = hline,
    ylab = ylab,
    yTicks = yTicks,
    prefix = paste( letters[alphaIndex], "goodScens", sep = "_" ),
    .agg = agg5 )
  
  cat( "\n Just finished marginal", y, ", oh yeah" )
  
  alphaIndex = alphaIndex + 1
  
}  # end loop over Y

# auto-generate figure strings for Overleaf
overleaf_figure_strings()


##### Marginal Violin Plots - All Scenarios #####

# this will be incremented so that each plot's title is prefaced
#  by a letter to force the correct ordering

for ( y in outcomes ) {
  
  yTicks = NA
  
  # set up y-labels and hline
  if ( y == "PhatRelBias" ) {
    ylab = "Relative bias in estimated proportion above q"
    hline = 0
    yTicks = seq(0, 5, .5)
    
    #cat("\\subsubsection{Relative bias in est prop}")
  }
  
  if ( y == "DiffRelBias" ) {
    ylab = "Relative bias in estimated difference in proportions"
    hline = 0
    yTicks = seq(0, 5, .5)
  }
  
  if ( y == "CoverPhat" ) {
    ylab = "95% CI coverage for estimated proportion above q"
    hline = 0.95
    yTicks = seq(0, 1, 0.1)
  }
  
  if ( y == "CoverDiff" ) {
    ylab = "95% CI coverage of estimated difference in proportions"
    hline = 0.95
    yTicks = seq(0, 1, 0.1)
  }
  
  my_violins( 
              yName = y,
              hline = hline,
              ylab = ylab,
              yTicks = yTicks,
              prefix = letters[alphaIndex] )
  
  cat( "\n Just finished marginal", y, ", oh yeah" )
  
  alphaIndex = alphaIndex + 1
  
}  # end loop over Y



# auto-generate figure strings for Overleaf
overleaf_figure_strings()



##### Violin Plots by Meta-Analysis Characteristics - All Scenarios #####


# simplified list of categorical observed variables
categX = c("k", "muN", "clustered.pretty", "true.effect.dist.pretty", "contrast", "TheoryP")
# also Phat, but it's continuous

# total number of plots 
length(outcomes) * length(categX)


# pretty variable names for X-axis
agg$true.effect.dist.pretty = dplyr::recode( agg$true.effect.dist,
                                             expo = "Exponential",
                                             normal = "Normal")

agg$clustered.pretty = dplyr::recode( agg$clustered,
                                      `0` = "Not clustered",
                                      `1` = "Clustered")


# #bm
# my_violins( xName = "k",
#             yName = "PhatRelBias",
#             hline = 0,
#             xlab = "X",
#             ylab = "Y",
#             yTicks = seq(0,1,.1),
#             prefix = LETTERS[3])


for ( y in outcomes ) {
  
  yTicks = NA
  
  # set up y-labels and hline
  if ( y == "PhatRelBias" ) {
    ylab = "Relative bias in estimated proportion above q"
    hline = 0
    yTicks = seq(0, 5, .5)
    
  }
  
  if ( y == "DiffRelBias" ) {
    ylab = "Relative bias in estimated difference in proportions"
    hline = 0
    yTicks = seq(0, 5, .5)
  }
  
  if ( y == "CoverPhat" ) {
    ylab = "95% CI coverage for estimated proportion above q"
    hline = 0.95
    yTicks = seq(0, 1, 0.1)
  }
  
  if ( y == "CoverDiff" ) {
    ylab = "95% CI coverage of estimated difference in proportions"
    hline = 0.95
    yTicks = seq(0, 1, 0.1)
  }
  
  
  for ( x in categX ) {
    
    # set up x-labels
    if ( x == "k" ) xlab = "Number of studies (k)"
    if ( x == "muN" ) xlab = "Average sample size (E[N])"
    if ( x == "EstVar" ) xlab = "Estimated residual heterogeneity"
    if ( x == "clustered.pretty" ) xlab = "Clustering of population effects"
    if ( x == "true.effect.dist.pretty" ) xlab = "Distribution of population effects"
    if ( x == "contrast" ) xlab = "Covariate contrast"
    
    my_violins( xName = x,
                yName = y,
                hline = hline,
                xlab = xlab,
                ylab = ylab,
                yTicks = yTicks,
                prefix = letters[alphaIndex] )
    
    cat( "\n Just finished", x, "vs", y, ", oh yeah" )
    
    alphaIndex = alphaIndex + 1
    
  }  # end loop over X
  
  
}  # end loop over Y


# auto-generate figure strings for Overleaf
overleaf_figure_strings()

################################## TABLES FOR PAPER, WITH RULES OF THUMB ##################################


# choose which average to take across scenarios ("median" or "mean")
averagefn = "median"


##### For Phat #####

# make filtered dfs (will form rows of table)
agg2 = make_agg_data( s %>% filter(contrast != "BC-rare") )
agg3 = make_agg_data( s %>% filter(true.effect.dist == "normal") )
# excluding only clustered expo is same as excluding all clustered
agg4 = make_agg_data( s %>% filter( !(clustered == TRUE & true.effect.dist == "expo") ) )

selectVars = "Phat"

t1 = rbind( my_summarise(dat = agg,
                         description = "All reps",
                         averagefn = averagefn),
            
            # **this one gives 0% chance of coverage<85%
            my_summarise(dat = agg3,
                         description = "Normal",
                         averagefn = averagefn),
            
            # **this one gives 1% chance of coverage<85%
            my_summarise(dat = agg4,
                         description = "Not clustered expo",
                         averagefn = averagefn) )  

View(t1)

# save pretty table for paper
keepers = c("Scenarios",
            "n.scens", 
            "PhatBias",
            "PhatAbsBias",
            "PhatRelBias",
            "EstMeanRelBias",
            "EstVarRelBias",
            "CoverPhat",
            "BadPhatCover")
setwd(results.dir)
setwd("Tables to prettify")
write.csv(t1 %>% select(keepers), "Phat_results_table.csv")


##### For Diff #####

agg5 = make_agg_data( s %>% filter(contrast != "BC-rare" &
                                     !(clustered == TRUE & true.effect.dist == "expo") ) )

# tried but not useful:
#agg5 = make_agg_data( s %>% filter(contrast != "BC-rare" & k>=100 ) )

# agg6 = make_agg_data(s %>% filter(k >= 100) )
# 
# agg7 = make_agg_data(s %>% filter( k >= 100 &
#                                      !(clustered == TRUE & true.effect.dist == "expo") ) ) 
# 
# agg9 = make_agg_data(s %>% filter( k >= 100 &
#                                      true.effect.dist == "normal" ) ) 
# 
# agg10 = make_agg_data(s %>% filter( k >= 100 &
#                                       muN==850 &
#                                       true.effect.dist == "normal" &
#                                       clustered == FALSE) ) 

selectVars = "Diff"

t2 = rbind( my_summarise(dat = agg,
                         description = "All reps",
                         averagefn = averagefn),
            
            my_summarise(dat = agg3,
                         description = "Normal",
                         averagefn = averagefn),
            
            
            my_summarise(dat = agg2,
                         description = "Not BC-rare",
                         averagefn = averagefn),
            
            
            my_summarise(dat = agg5,
                         description = "Not BC-rare nor clustered expo",
                         averagefn = averagefn)
) 
View(t2)

# save pretty table for paper
keepers = c("Scenarios",
            "n.scens", 
            "DiffBias",
            "DiffAbsBias",
            "DiffRelBias",
            "EstMeanRelBias",
            "EstVarRelBias",
            "CoverDiff",
            "BadDiffCover")
setwd(results.dir)
setwd("Tables to prettify")
write.csv(t2 %>% select(keepers), "diff_results_table.csv")



###### All Metrics for All of the Filtered Datasets (Online Dataset) #####

# diff, phat, logitphat, etc.

selectVars = "all"
t3 = rbind( my_summarise(dat = agg,
                         description = "All reps",
                         averagefn = averagefn),
            
            my_summarise(dat = agg2,
                         description = "No BC-rare",
                         averagefn = averagefn),
            
            my_summarise(dat = agg3,
                         description = "Normal",
                         averagefn = averagefn),
            
            my_summarise(dat = agg4,
                         description = "Not clustered expo",
                         averagefn = averagefn),
            
            my_summarise(dat = agg5,
                         description = "No BC-rare/not clustered expo",
                         averagefn = averagefn) ) 


setwd(results.dir)
write.csv(t3, "extended_performance_table.csv")



################################## BIAS-CORRECT META-REGRESSION ESTIMATES IN WORST SCENARIOS ##################################

setwd("~/Dropbox/Personal computer/Independent studies/2020/Meta-regression metrics (MRM)/Simulation study results/Supplement - boot-correct meta-regression")

# "b" for "bias correction"
s2b = fread("stitched.csv")
s2b$contrast = "BC-rare"

s3b = make_s3_data(s2b)


# has one row per scenario, so 3 rows per scen.name.in.main
aggb = make_agg_data(s3b)

selectVars = "all"

t = data.frame( rbind( my_summarise(dat = aggb %>% filter(calib.method == "MR"),
                                    description = "MR"),
                       
                       my_summarise(dat = aggb %>% filter(calib.method == "MR bt both correct"),
                                    description = "Bt correct"),
                       
                       my_summarise(dat = aggb %>% filter(calib.method == "params"),
                                    description = "params") ) )

t = t %>% select("Scenarios", "PhatBias", "PhatAbsBias",  "PhatRelBias", "DiffBias", "DiffAbsBias", "DiffRelBias")
t

# yes, the bias corrections make the bias worse...

setwd(results.dir)
setwd("Tables to prettify")
fwrite(t, "supplement_meta_regression_corrections.csv")






